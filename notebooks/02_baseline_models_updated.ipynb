{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models for Killer Prediction (Updated)\n",
    "\n",
    "This notebook runs and analyzes baseline models using the refactored data models.\n",
    "\n",
    "## Key Updates:\n",
    "- Uses proper data models (Episode, Character, Sentence)\n",
    "- Supports both character modes (episode-isolated and cross-episode)\n",
    "- Consistent with refactored analysis modules\n",
    "\n",
    "## Models Implemented:\n",
    "1. **Frequency-based baselines**: Simple heuristics based on speaking patterns\n",
    "2. **Traditional ML baselines**: Classic NLP approaches without deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Import refactored baseline models\n",
    "from analysis.baseline_models import BaselineModels\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Baseline Models with Character Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose character mode\n",
    "CHARACTER_MODE = 'episode-isolated'  # or 'cross-episode'\n",
    "\n",
    "# Initialize baseline models with proper data models\n",
    "baselines = BaselineModels(\n",
    "    data_dir=Path('../data/original'),\n",
    "    character_mode=CHARACTER_MODE\n",
    ")\n",
    "\n",
    "print(f\"Character mode: {CHARACTER_MODE}\")\n",
    "print(f\"Loaded {len(baselines.episodes)} episodes\")\n",
    "print(f\"Total unique characters: {baselines.summary_stats['unique_characters']}\")\n",
    "print(f\"\\nSample episodes:\")\n",
    "\n",
    "for i, episode in enumerate(baselines.episodes[:5]):\n",
    "    char_data = baselines.get_episode_characters(episode.episode_id)\n",
    "    killers = baselines.character_labels.get(episode.episode_id, set())\n",
    "    print(f\"  {episode.episode_id}: {len(episode.sentences)} sentences, \"\n",
    "          f\"{len(char_data)} characters, {len(killers)} killer(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Individual Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency baseline\n",
    "freq_results = baselines.frequency_baseline(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appearance order baseline\n",
    "appearance_results = baselines.appearance_order_baseline(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words + Logistic Regression\n",
    "bow_results = baselines.bow_logistic_regression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + SVM\n",
    "tfidf_results = baselines.tfidf_svm(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram features\n",
    "ngram_results = baselines.ngram_features_baseline(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined features (statistical + text)\n",
    "combined_results = baselines.combined_features_baseline(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run All Baselines and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all baselines and get summary\n",
    "results_df = baselines.run_all_baselines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Both Character Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across both character modes\n",
    "mode_results = {}\n",
    "\n",
    "for mode in ['episode-isolated', 'cross-episode']:\n",
    "    print(f\"\\nRunning baselines for {mode} mode...\")\n",
    "    mode_baselines = BaselineModels(\n",
    "        data_dir=Path('../data/original'),\n",
    "        character_mode=mode\n",
    "    )\n",
    "    mode_results[mode] = mode_baselines.run_all_baselines()\n",
    "    print(f\"Completed {mode} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, (mode, df) in enumerate(mode_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    df.plot(x='Model', y='F1', kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_title(f'F1 Scores - {mode} mode', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(df['F1']):\n",
    "        ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Baseline Performance: Episode-Isolated vs Cross-Episode', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison summary\n",
    "print(\"\\nBest models by mode:\")\n",
    "for mode, df in mode_results.items():\n",
    "    best_idx = df['F1'].idxmax()\n",
    "    print(f\"  {mode}: {df.loc[best_idx, 'Model']} (F1={df.loc[best_idx, 'F1']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on one mode for detailed visualization\n",
    "df = results_df  # Use the initial results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "df.plot(x='Model', y='Accuracy', kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision/Recall comparison\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, df['Precision'], width, label='Precision', color='green', alpha=0.7)\n",
    "ax.bar(x + width/2, df['Recall'], width, label='Recall', color='orange', alpha=0.7)\n",
    "ax.set_title('Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "ax = axes[1, 0]\n",
    "df.plot(x='Model', y='F1', kind='bar', ax=ax, color='purple')\n",
    "ax.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation stability (for ML models)\n",
    "ax = axes[1, 1]\n",
    "ml_models = df[df['CV_Std'].notna()]\n",
    "if not ml_models.empty:\n",
    "    x = np.arange(len(ml_models))\n",
    "    ax.bar(x, ml_models['Accuracy'], yerr=ml_models['CV_Std'], \n",
    "           capsize=5, alpha=0.7, color='teal')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ml_models['Model'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Cross-Validation Stability', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Baseline Model Performance ({CHARACTER_MODE} mode)', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze important features from BoW model\n",
    "if bow_results.feature_importance:\n",
    "    print(f\"Top Killer-Indicative Words ({CHARACTER_MODE} mode):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_features = sorted(bow_results.feature_importance.items(), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    words = [w for w, _ in sorted_features]\n",
    "    scores = [s for _, s in sorted_features]\n",
    "    \n",
    "    y_pos = np.arange(len(words))\n",
    "    ax.barh(y_pos, scores, color='crimson', alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(words)\n",
    "    ax.set_xlabel('Feature Importance Score')\n",
    "    ax.set_title(f'Top Words Associated with Killers ({CHARACTER_MODE} mode)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"• These words appear more frequently in killer dialogue\")\n",
    "    print(\"• Higher scores indicate stronger association with killer class\")\n",
    "    print(\"• This provides linguistic insights into killer speech patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(f\"BASELINE ANALYSIS SUMMARY ({CHARACTER_MODE} mode)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DATA STATISTICS:\")\n",
    "print(f\"   • Total episodes: {len(baselines.episodes)}\")\n",
    "print(f\"   • Total unique characters: {baselines.summary_stats['unique_characters']}\")\n",
    "print(f\"   • Total sentences: {baselines.summary_stats['total_sentences']}\")\n",
    "print(f\"   • Avg sentences/episode: {baselines.summary_stats['avg_sentences_per_episode']:.1f}\")\n",
    "print(f\"   • Avg characters/episode: {baselines.summary_stats['avg_characters_per_episode']:.1f}\")\n",
    "\n",
    "print(\"\\n2. OVERALL PERFORMANCE:\")\n",
    "print(f\"   • Best model: {results_df.loc[results_df['F1'].idxmax(), 'Model']}\")\n",
    "print(f\"   • Best F1 score: {results_df['F1'].max():.3f}\")\n",
    "print(f\"   • Average F1 across all models: {results_df['F1'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n3. MODEL TYPE COMPARISON:\")\n",
    "simple_models = ['Frequency Baseline', 'Appearance Order Baseline']\n",
    "ml_models = ['BoW + Logistic Regression', 'TF-IDF + SVM', 'N-gram Features', 'Combined Features']\n",
    "simple_perf = results_df[results_df['Model'].isin(simple_models)]['F1'].mean()\n",
    "ml_perf = results_df[results_df['Model'].isin(ml_models)]['F1'].mean()\n",
    "\n",
    "print(f\"   • Simple heuristics avg F1: {simple_perf:.3f}\")\n",
    "print(f\"   • Traditional ML avg F1: {ml_perf:.3f}\")\n",
    "print(f\"   • Improvement: {(ml_perf - simple_perf):.3f} ({(ml_perf - simple_perf)/simple_perf*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. KEY FINDINGS:\")\n",
    "if results_df['F1'].max() > 0.5:\n",
    "    print(f\"   ✓ Killer prediction is significantly better than random\")\n",
    "if ml_perf > simple_perf:\n",
    "    print(f\"   ✓ Text features improve prediction over simple heuristics\")\n",
    "if CHARACTER_MODE == 'episode-isolated':\n",
    "    print(f\"   • Characters are treated independently per episode\")\n",
    "else:\n",
    "    print(f\"   • Characters are consolidated across episodes\")\n",
    "\n",
    "print(\"\\n5. IMPLICATIONS FOR NEURAL APPROACH:\")\n",
    "print(f\"   • Baseline to beat: F1={results_df['F1'].max():.3f}\")\n",
    "print(f\"   • Neural embeddings should capture richer semantic patterns\")\n",
    "print(f\"   • Character mode '{CHARACTER_MODE}' provides the baseline context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results for Later Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for both modes\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = Path(f'../experiments/baseline_results_{CHARACTER_MODE}')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save detailed results\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'character_mode': CHARACTER_MODE,\n",
    "    'data_stats': baselines.summary_stats,\n",
    "    'best_model': results_df.loc[results_df['F1'].idxmax(), 'Model'],\n",
    "    'best_f1': float(results_df['F1'].max()),\n",
    "    'average_f1': float(results_df['F1'].mean()),\n",
    "    'simple_heuristics_avg': float(simple_perf),\n",
    "    'traditional_ml_avg': float(ml_perf),\n",
    "    'all_results': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open(output_dir / 'baseline_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Save DataFrame\n",
    "results_df.to_csv(output_dir / 'baseline_scores.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for file in output_dir.glob('*'):\n",
    "    print(f\"  • {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}