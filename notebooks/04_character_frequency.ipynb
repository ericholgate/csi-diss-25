{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Frequency Analysis\n",
    "\n",
    "Comprehensive analysis of character appearances, frequencies, and interaction patterns.\n",
    "\n",
    "## Analysis Components:\n",
    "1. **Character Frequency**: How often each character appears and speaks\n",
    "2. **Character Types**: Main, recurring, guest, and minor characters\n",
    "3. **Co-occurrence Patterns**: Which characters appear together\n",
    "4. **Network Analysis**: Character interaction networks\n",
    "5. **Timeline Analysis**: Character appearance patterns over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Import character frequency analyzer\n",
    "from analysis.character_frequency_analyzer import CharacterFrequencyAnalyzer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Analyzer and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = CharacterFrequencyAnalyzer(data_dir=Path('../data/original'))\n",
    "\n",
    "# Get character type breakdown\n",
    "char_types = analyzer.identify_character_types()\n",
    "\n",
    "print(\"Character Database Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total unique characters: {len(analyzer.character_profiles)}\")\n",
    "print(f\"Total episodes analyzed: {len(analyzer.episodes)}\")\n",
    "print(f\"\\nCharacter Type Breakdown:\")\n",
    "for char_type, chars in char_types.items():\n",
    "    print(f\"  {char_type.capitalize()}: {len(chars)} characters\")\n",
    "    if char_type == 'main' and chars:\n",
    "        print(f\"    Examples: {', '.join(chars[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Character Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive character distributions\n",
    "fig = analyzer.plot_character_distributions(top_n=20)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"• Characters appearing in 10+ episodes: {sum(1 for p in analyzer.character_profiles.values() if p.episode_count >= 10)}\")\n",
    "print(f\"• Characters appearing in only 1 episode: {sum(1 for p in analyzer.character_profiles.values() if p.episode_count == 1)}\")\n",
    "print(f\"• Average sentences per character: {np.mean([p.total_sentences for p in analyzer.character_profiles.values()]):.1f}\")\n",
    "print(f\"• Max sentences by one character: {max(p.total_sentences for p in analyzer.character_profiles.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top Characters Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rankings by different metrics\n",
    "by_sentences = analyzer.get_character_rankings('total_sentences')[:15]\n",
    "by_episodes = analyzer.get_character_rankings('episode_count')[:15]\n",
    "by_intensity = analyzer.get_character_rankings('avg_sentences')[:15]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "rankings_df = pd.DataFrame({\n",
    "    'By Total Sentences': [f\"{name} ({count})\" for name, count in by_sentences],\n",
    "    'By Episode Count': [f\"{name} ({count})\" for name, count in by_episodes],\n",
    "    'By Avg Sentences/Episode': [f\"{name} ({count:.1f})\" for name, count in by_intensity]\n",
    "})\n",
    "\n",
    "display(HTML(\"<h3>Character Rankings by Different Metrics</h3>\"))\n",
    "display(rankings_df.head(10).style.set_properties(**{'text-align': 'left'}))\n",
    "\n",
    "# Identify CSI team members\n",
    "csi_team_keywords = ['grissom', 'sara', 'nick', 'warrick', 'catherine', 'greg', 'brass']\n",
    "csi_team = []\n",
    "for name, profile in analyzer.character_profiles.items():\n",
    "    if any(keyword in name.lower() for keyword in csi_team_keywords):\n",
    "        if profile.episode_count >= 5:  # Filter to significant appearances\n",
    "            csi_team.append((profile.name, profile.episode_count, profile.total_sentences))\n",
    "\n",
    "if csi_team:\n",
    "    print(\"\\nIdentified CSI Team Members:\")\n",
    "    for name, eps, sents in sorted(csi_team, key=lambda x: x[2], reverse=True):\n",
    "        print(f\"  {name}: {eps} episodes, {sents} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Character Co-occurrence Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze co-occurrence patterns\n",
    "co_occur = analyzer.analyze_co_occurrences(min_weight=2)\n",
    "\n",
    "if 'error' not in co_occur:\n",
    "    print(\"Co-occurrence Network Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Network nodes: {co_occur['nodes']}\")\n",
    "    print(f\"Network edges: {co_occur['edges']}\")\n",
    "    print(f\"Network density: {co_occur['density']:.3f}\")\n",
    "    print(f\"Average clustering: {co_occur['avg_clustering']:.3f}\")\n",
    "    print(f\"Connected components: {co_occur['connected_components']}\")\n",
    "    \n",
    "    if 'most_connected' in co_occur:\n",
    "        print(\"\\nMost Connected Characters (Hub Characters):\")\n",
    "        for char, centrality in co_occur['most_connected'][:8]:\n",
    "            char_name = analyzer.character_profiles[char].name\n",
    "            print(f\"  {char_name}: {centrality:.3f}\")\n",
    "    \n",
    "    if 'frequent_pairs' in co_occur:\n",
    "        print(\"\\nMost Frequent Character Pairs:\")\n",
    "        for char1, char2, weight in co_occur['frequent_pairs'][:10]:\n",
    "            name1 = analyzer.character_profiles[char1].name\n",
    "            name2 = analyzer.character_profiles[char2].name\n",
    "            print(f\"  {name1} & {name2}: {weight} episodes together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize co-occurrence network\n",
    "fig = analyzer.plot_co_occurrence_network(min_weight=3)\n",
    "\n",
    "if fig:\n",
    "    print(\"\\nNetwork Visualization Notes:\")\n",
    "    print(\"• Node size = character speaking frequency\")\n",
    "    print(\"• Node color: Gold=Main, Blue=Recurring, Gray=Guest/Minor\")\n",
    "    print(\"• Edge thickness = frequency of co-appearance\")\n",
    "    print(\"• Closer nodes = more frequent co-appearances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Character Timeline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze timeline for main characters\n",
    "main_chars = char_types['main'][:5] if 'main' in char_types else []\n",
    "\n",
    "if main_chars:\n",
    "    fig, axes = plt.subplots(len(main_chars), 1, figsize=(14, 3*len(main_chars)))\n",
    "    if len(main_chars) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, char_name in enumerate(main_chars):\n",
    "        # Get timeline\n",
    "        timeline = analyzer.get_character_timeline(char_name)\n",
    "        \n",
    "        if 'error' not in timeline and 'episode_details' in timeline:\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Extract episode numbers and sentences\n",
    "            episodes = []\n",
    "            sentences = []\n",
    "            \n",
    "            for detail in timeline['episode_details']:\n",
    "                ep = detail['episode']\n",
    "                if 'e' in ep:\n",
    "                    try:\n",
    "                        ep_num = int(ep.split('e')[1])\n",
    "                        episodes.append(ep_num)\n",
    "                        sentences.append(detail['sentences'])\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if episodes:\n",
    "                ax.bar(episodes, sentences, color='steelblue', alpha=0.7)\n",
    "                ax.set_xlabel('Episode Number')\n",
    "                ax.set_ylabel('Sentences')\n",
    "                ax.set_title(f'{timeline[\"character\"]} Speaking Pattern ({timeline[\"total_episodes\"]} episodes, {timeline[\"total_sentences\"]} total sentences)')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add average line\n",
    "                avg_sentences = np.mean(sentences)\n",
    "                ax.axhline(y=avg_sentences, color='red', linestyle='--', alpha=0.5,\n",
    "                          label=f'Average: {avg_sentences:.1f}')\n",
    "                ax.legend()\n",
    "    \n",
    "    plt.suptitle('Main Character Speaking Patterns Across Episodes', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No main characters identified for timeline analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Character Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate character similarity based on co-occurrence\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Get main and recurring characters only\n",
    "significant_chars = []\n",
    "for name, profile in analyzer.character_profiles.items():\n",
    "    if profile.episode_count >= 5:  # At least 5 episodes\n",
    "        significant_chars.append(name)\n",
    "\n",
    "if len(significant_chars) >= 10:\n",
    "    # Create similarity matrix for significant characters\n",
    "    char_indices = {char: i for i, char in enumerate(significant_chars)}\n",
    "    n_chars = len(significant_chars)\n",
    "    similarity_matrix = np.zeros((n_chars, n_chars))\n",
    "    \n",
    "    for i, char1 in enumerate(significant_chars):\n",
    "        for j, char2 in enumerate(significant_chars):\n",
    "            if i == j:\n",
    "                similarity_matrix[i, j] = 1.0\n",
    "            else:\n",
    "                # Count co-appearances\n",
    "                co_count = analyzer.character_profiles[char1].co_appearances.get(char2, 0)\n",
    "                # Normalize by minimum episodes\n",
    "                min_eps = min(analyzer.character_profiles[char1].episode_count,\n",
    "                            analyzer.character_profiles[char2].episode_count)\n",
    "                similarity_matrix[i, j] = co_count / max(1, min_eps)\n",
    "    \n",
    "    # Create dendrogram\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Convert similarity to distance\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(distance_matrix[np.triu_indices(n_chars, k=1)], method='ward')\n",
    "    \n",
    "    # Create dendrogram\n",
    "    char_labels = [analyzer.character_profiles[c].name for c in significant_chars]\n",
    "    dendrogram(linkage_matrix, labels=char_labels, ax=ax, orientation='right')\n",
    "    \n",
    "    ax.set_xlabel('Distance (1 - Co-appearance Rate)')\n",
    "    ax.set_title('Character Clustering Based on Co-appearances', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Dendrogram Interpretation:\")\n",
    "    print(\"• Characters that frequently appear together cluster together\")\n",
    "    print(\"• Shorter branches = more similar co-appearance patterns\")\n",
    "    print(\"• Distinct clusters may represent different storylines or teams\")\n",
    "else:\n",
    "    print(\"Not enough significant characters for clustering analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Episode Character Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character diversity per episode\n",
    "episode_diversity = []\n",
    "for ep_id, stats in analyzer.episode_stats.items():\n",
    "    episode_diversity.append({\n",
    "        'episode': ep_id,\n",
    "        'character_count': stats.character_count,\n",
    "        'total_sentences': stats.total_sentences,\n",
    "        'avg_sentences_per_char': stats.total_sentences / max(1, stats.character_count)\n",
    "    })\n",
    "\n",
    "diversity_df = pd.DataFrame(episode_diversity)\n",
    "\n",
    "# Visualize episode diversity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Character count distribution\n",
    "ax = axes[0]\n",
    "ax.hist(diversity_df['character_count'], bins=15, edgecolor='black', alpha=0.7, color='green')\n",
    "ax.set_xlabel('Number of Characters')\n",
    "ax.set_ylabel('Number of Episodes')\n",
    "ax.set_title('Characters per Episode Distribution')\n",
    "ax.axvline(x=diversity_df['character_count'].mean(), color='red', linestyle='--',\n",
    "           label=f'Mean: {diversity_df[\"character_count\"].mean():.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Sentences per episode\n",
    "ax = axes[1]\n",
    "ax.hist(diversity_df['total_sentences'], bins=15, edgecolor='black', alpha=0.7, color='blue')\n",
    "ax.set_xlabel('Total Sentences')\n",
    "ax.set_ylabel('Number of Episodes')\n",
    "ax.set_title('Dialogue Density Distribution')\n",
    "ax.axvline(x=diversity_df['total_sentences'].mean(), color='red', linestyle='--',\n",
    "           label=f'Mean: {diversity_df[\"total_sentences\"].mean():.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Relationship between characters and sentences\n",
    "ax = axes[2]\n",
    "ax.scatter(diversity_df['character_count'], diversity_df['total_sentences'],\n",
    "          alpha=0.6, s=50)\n",
    "ax.set_xlabel('Number of Characters')\n",
    "ax.set_ylabel('Total Sentences')\n",
    "ax.set_title('Characters vs Dialogue Volume')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(diversity_df['character_count'], diversity_df['total_sentences'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(diversity_df['character_count'].min(), \n",
    "                     diversity_df['character_count'].max(), 100)\n",
    "ax.plot(x_trend, p(x_trend), 'r--', alpha=0.5, label=f'Trend: {z[0]:.1f}x + {z[1]:.1f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Episode Character Diversity Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEpisode Diversity Statistics:\")\n",
    "print(f\"• Average characters per episode: {diversity_df['character_count'].mean():.1f}\")\n",
    "print(f\"• Average sentences per episode: {diversity_df['total_sentences'].mean():.1f}\")\n",
    "print(f\"• Average sentences per character: {diversity_df['avg_sentences_per_char'].mean():.1f}\")\n",
    "print(f\"• Most crowded episode: {diversity_df.loc[diversity_df['character_count'].idxmax(), 'episode']} ({diversity_df['character_count'].max()} characters)\")\n",
    "print(f\"• Most dialogue: {diversity_df.loc[diversity_df['total_sentences'].idxmax(), 'episode']} ({diversity_df['total_sentences'].max()} sentences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Character Communities Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect character communities\n",
    "if 'communities' in co_occur and co_occur['communities']:\n",
    "    print(\"Detected Character Communities:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, community in enumerate(co_occur['communities'][:5], 1):\n",
    "        print(f\"\\nCommunity {i} ({community['size']} members):\")\n",
    "        # Show first 10 members\n",
    "        members_to_show = community['members'][:10]\n",
    "        for member in members_to_show:\n",
    "            print(f\"  • {member}\")\n",
    "        if len(community['members']) > 10:\n",
    "            print(f\"  ... and {len(community['members']) - 10} more\")\n",
    "    \n",
    "    print(\"\\nCommunity Interpretation:\")\n",
    "    print(\"• Communities represent groups of characters that frequently appear together\")\n",
    "    print(\"• May correspond to different storylines, teams, or social groups\")\n",
    "    print(\"• Larger communities indicate core character groups\")\n",
    "else:\n",
    "    print(\"No significant character communities detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save comprehensive report\n",
    "output_dir = Path('../experiments/character_analysis')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report = analyzer.generate_report(save_path=output_dir / 'character_frequency.json')\n",
    "\n",
    "# Create summary markdown\n",
    "summary_md = f\"\"\"\n",
    "# Character Frequency Analysis Summary\n",
    "\n",
    "## Dataset Overview\n",
    "- **Total Unique Characters**: {report['overview']['total_characters']}\n",
    "- **Total Episodes**: {report['overview']['total_episodes']}\n",
    "- **Main Characters**: {report['overview']['main_characters']}\n",
    "- **Recurring Characters**: {report['overview']['recurring_characters']}\n",
    "\n",
    "## Episode Statistics\n",
    "- **Avg Characters per Episode**: {report['episode_statistics']['avg_characters_per_episode']:.1f}\n",
    "- **Character Range**: {report['episode_statistics']['min_characters']}-{report['episode_statistics']['max_characters']}\n",
    "- **Avg Sentences per Episode**: {report['episode_statistics']['avg_sentences_per_episode']:.1f}\n",
    "\n",
    "## Top 5 Characters by Total Sentences\n",
    "\"\"\"\n",
    "\n",
    "for name, count in report['top_characters']['by_sentences'][:5]:\n",
    "    summary_md += f\"1. **{name}**: {count} sentences\\n\"\n",
    "\n",
    "summary_md += f\"\"\"\n",
    "\n",
    "## Network Analysis\n",
    "- **Network Nodes**: {co_occur.get('nodes', 'N/A')}\n",
    "- **Network Edges**: {co_occur.get('edges', 'N/A')}\n",
    "- **Network Density**: {co_occur.get('density', 0):.3f}\n",
    "- **Connected Components**: {co_occur.get('connected_components', 'N/A')}\n",
    "\n",
    "## Implications for Character Embeddings\n",
    "1. **Main characters** provide most training data ({report['overview']['main_characters']} characters)\n",
    "2. **Character communities** suggest natural embedding clusters\n",
    "3. **Co-occurrence patterns** will influence embedding similarity\n",
    "4. **Episode diversity** ({report['episode_statistics']['avg_characters_per_episode']:.1f} chars/episode) ensures varied contexts\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary_md))\n",
    "\n",
    "# Save summary\n",
    "with open(output_dir / 'character_summary.md', 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(f\"\\nResults saved to {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "for file in output_dir.glob('*'):\n",
    "    print(f\"  • {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}